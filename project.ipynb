{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXJWuagQEiBD2PldqFMJV9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a51060Nirnaya/PythonForDataScience/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised model\n"
      ],
      "metadata": {
        "id": "ZrxOA_as7s7o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBXwLtXF7dNH",
        "outputId": "728f8ae8-39b2-423c-c038-860bba335b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Logistic Regression =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.51      0.52      0.51      2011\n",
            "        Real       0.50      0.48      0.49      1989\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "\n",
            "===== Decision Tree =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.49      0.51      0.50      2011\n",
            "        Real       0.48      0.46      0.47      1989\n",
            "\n",
            "    accuracy                           0.49      4000\n",
            "   macro avg       0.49      0.49      0.49      4000\n",
            "weighted avg       0.49      0.49      0.49      4000\n",
            "\n",
            "\n",
            "===== Random Forest =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.50      0.56      0.53      2011\n",
            "        Real       0.49      0.43      0.46      1989\n",
            "\n",
            "    accuracy                           0.49      4000\n",
            "   macro avg       0.49      0.49      0.49      4000\n",
            "weighted avg       0.49      0.49      0.49      4000\n",
            "\n",
            "\n",
            "===== KNN =====\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.51      0.53      0.52      2011\n",
            "        Real       0.51      0.49      0.50      1989\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.51      4000\n",
            "weighted avg       0.51      0.51      0.51      4000\n",
            "\n",
            "\n",
            "===== SVM =====\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Supervised learning models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/fake_news_dataset.csv\")\n",
        "\n",
        "# Combine title + text for better feature representation\n",
        "df[\"content\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
        "\n",
        "# Features (X) and Labels (y)\n",
        "X = df[\"content\"]\n",
        "y = df[\"label\"].map({\"real\": 1, \"fake\": 0})  # Encode labels\n",
        "\n",
        "# Drop rows with NaN values in the 'label' column\n",
        "nan_rows = y.isna()\n",
        "X = X[~nan_rows]\n",
        "y = y[~nan_rows]\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Convert text into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Fake\", \"Real\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#unsupervised model\n"
      ],
      "metadata": {
        "id": "EUO5WSIL8H9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "\n",
        "# Unsupervised learning models\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/fake_news_dataset.csv\")\n",
        "\n",
        "# Combine title + text for better feature representation\n",
        "df[\"content\"] = df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)\n",
        "\n",
        "# Features (X) and Labels (y)\n",
        "X = df[\"content\"]\n",
        "y = df[\"label\"].map({\"real\": 1, \"fake\": 0})  # Encode labels\n",
        "\n",
        "# Convert text into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"KMeans\": KMeans(n_clusters=2, random_state=42),\n",
        "    \"Agglomerative Clustering\": AgglomerativeClustering(n_clusters=2),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.7, min_samples=5),\n",
        "    \"Gaussian Mixture\": GaussianMixture(n_components=2, random_state=42)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Train and evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "\n",
        "    if name in [\"Agglomerative Clustering\", \"DBSCAN\"]:\n",
        "        labels = model.fit_predict(X_tfidf.toarray())\n",
        "    elif name == \"Gaussian Mixture\":\n",
        "        model.fit(X_tfidf.toarray())\n",
        "        labels = model.predict(X_tfidf.toarray())\n",
        "    else:  # KMeans supports sparse input\n",
        "        labels = model.fit_predict(X_tfidf)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    ari = adjusted_rand_score(y, labels)\n",
        "    sil = silhouette_score(X_tfidf, labels) if len(set(labels)) > 1 else -1\n",
        "\n",
        "    print(f\"Adjusted Rand Index (vs true labels): {ari:.4f}\")\n",
        "    print(f\"Silhouette Score: {sil:.4f}\")\n",
        "\n",
        "    results.append({\"Model\": name, \"ARI\": ari, \"Silhouette\": sil})\n",
        "\n",
        "# Create summary table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n===== Summary of Unsupervised Learning Results =====\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "ms7zh-Dc8MVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep learning model\n"
      ],
      "metadata": {
        "id": "w-sTj3Py8v0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Deep Learning models for Fake News Classification\n",
        "Colab-ready script. Implements multiple deep learning approaches:\n",
        " 1) Dense Neural Network on TF-IDF features\n",
        " 2) 1D CNN on tokenized sequences\n",
        " 3) Bidirectional LSTM on tokenized sequences\n",
        " 4) (Optional) Transformer using Hugging Face (commented; requires internet/GPU)\n",
        "\n",
        "Outputs classification reports for each model.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Embedding, LSTM, Bidirectional, Input,\n",
        "                                     GlobalAveragePooling1D, SpatialDropout1D)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# --------- Config ---------\n",
        "DATA_PATH = '/content/fake_news_dataset.csv'\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "MAX_VOCAB = 30000\n",
        "MAX_LEN = 200  # max tokens for sequences\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 6\n",
        "TFIDF_MAX_FEATURES = 5000\n",
        "\n",
        "# GPU check\n",
        "print('TensorFlow version:', tf._version_)\n",
        "print('GPU available:', tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# --------- Load data ---------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df['content'] = df['title'].astype(str) + ' ' + df['text'].astype(str)\n",
        "texts = df['content'].astype(str).values\n",
        "labels = df['label'].astype(str).values\n",
        "\n",
        "# Encode labels (fake=0, real=1)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    texts, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Model 1: Dense on TF-IDF\n",
        "# -------------------------\n",
        "print('\\nPreparing TF-IDF features...')\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=TFIDF_MAX_FEATURES)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = vectorizer.transform(X_test_text)\n",
        "\n",
        "# Convert to dense if necessary (Keras Dense expects dense)\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_dense_tfidf(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(1e-4)),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print('\\nBuilding and training Dense (TF-IDF) model...')\n",
        "dense_model = build_dense_tfidf(X_train_tfidf_dense.shape[1])\n",
        "dense_ckpt = 'dense_tfidf_best.h5'\n",
        "cb_dense = [EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "            ModelCheckpoint(dense_ckpt, save_best_only=True, monitor='val_loss')]\n",
        "\n",
        "dense_model.fit(X_train_tfidf_dense, y_train, validation_split=0.1, epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE, callbacks=cb_dense, verbose=2)\n",
        "\n",
        "y_pred_dense = (dense_model.predict(X_test_tfidf_dense) > 0.5).astype(int).flatten()\n",
        "print('\\nDense (TF-IDF) Classification Report:')\n",
        "print(classification_report(y_test, y_pred_dense, target_names=le.classes_))\n",
        "\n",
        "# --------------------------------\n",
        "# Text tokenization for sequence models\n",
        "# --------------------------------\n",
        "print('\\nTokenizing texts for sequence models...')\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "vocab_size = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\n",
        "print('Vocab size used:', vocab_size)\n",
        "\n",
        "# -------------------------\n",
        "# Model 2: 1D CNN\n",
        "# -------------------------\n",
        "\n",
        "def build_cnn(vocab_size, embed_dim=EMBEDDING_DIM, input_length=MAX_LEN):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=input_length))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print('\\nBuilding and training 1D CNN model...')\n",
        "cnn_model = build_cnn(vocab_size)\n",
        "cnn_ckpt = 'cnn_best.h5'\n",
        "cb_cnn = [EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "          ModelCheckpoint(cnn_ckpt, save_best_only=True, monitor='val_loss')]\n",
        "\n",
        "cnn_model.fit(X_train_pad, y_train, validation_split=0.1, epochs=EPOCHS,\n",
        "              batch_size=BATCH_SIZE, callbacks=cb_cnn, verbose=2)\n",
        "\n",
        "y_pred_cnn = (cnn_model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
        "print('\\nCNN Classification Report:')\n",
        "print(classification_report(y_test, y_pred_cnn, target_names=le.classes_))\n",
        "\n",
        "# -------------------------\n",
        "# Model 3: Bidirectional LSTM\n",
        "# -------------------------\n",
        "\n",
        "def build_bilstm(vocab_size, embed_dim=EMBEDDING_DIM, input_length=MAX_LEN):\n",
        "    inp = Input(shape=(input_length,))\n",
        "    x = Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=input_length)(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=False))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    out = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print('\\nBuilding and training Bidirectional LSTM model...')\n",
        "lstm_model = build_bilstm(vocab_size)\n",
        "lstm_ckpt = 'bilstm_best.h5'\n",
        "cb_lstm = [EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
        "          ModelCheckpoint(lstm_ckpt, save_best_only=True, monitor='val_loss')]\n",
        "\n",
        "lstm_model.fit(X_train_pad, y_train, validation_split=0.1, epochs=EPOCHS,\n",
        "               batch_size=BATCH_SIZE, callbacks=cb_lstm, verbose=2)\n",
        "\n",
        "y_pred_lstm = (lstm_model.predict(X_test_pad) > 0.5).astype(int).flatten()\n",
        "print('\\nBiLSTM Classification Report:')\n",
        "print(classification_report(y_test, y_pred_lstm, target_names=le.classes_))\n",
        "\n",
        "# -------------------------\n",
        "# Optional: Transformer (Hugging Face)\n",
        "# -------------------------\n",
        "# NOTE: This section is optional and commented out because it requires internet\n",
        "# to download pretrained models and a GPU for practical training.\n",
        "# Uncomment and run in Colab if you have internet access and want to use BERT/RoBERTa.\n",
        "\n",
        "# # !pip install transformers datasets\n",
        "# from transformers import TFAutoModel, AutoTokenizer\n",
        "# PRETRAINED = 'distilbert-base-uncased'\n",
        "# tokenizer_hf = AutoTokenizer.from_pretrained(PRETRAINED)\n",
        "# def encode_texts(texts, tokenizer, max_len=MAX_LEN):\n",
        "#     enc = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_len, return_tensors='np')\n",
        "#     return enc['input_ids'], enc['attention_mask']\n",
        "#\n",
        "# input_ids_train, att_mask_train = encode_texts(X_train_text, tokenizer_hf, max_len=128)\n",
        "# input_ids_test, att_mask_test = encode_texts(X_test_text, tokenizer_hf, max_len=128)\n",
        "#\n",
        "# bert_model = TFAutoModel.from_pretrained(PRETRAINED)\n",
        "# input_ids = Input(shape=(128,), dtype='int32')\n",
        "# attention_mask = Input(shape=(128,), dtype='int32')\n",
        "# hidden = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "# pooled = GlobalAveragePooling1D()(hidden)\n",
        "# out = Dense(1, activation='sigmoid')(pooled)\n",
        "# model_bert = Model(inputs=[input_ids, attention_mask], outputs=out)\n",
        "# model_bert.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(2e-5), metrics=['accuracy'])\n",
        "#\n",
        "# model_bert.fit([input_ids_train, att_mask_train], y_train, validation_split=0.1, epochs=3, batch_size=16)\n",
        "# y_pred_bert = (model_bert.predict([input_ids_test, att_mask_test]) > 0.5).astype(int).flatten()\n",
        "# print('\\nTransformer (pretrained) Classification Report:')\n",
        "# print(classification_report(y_test, y_pred_bert, target_names=le.classes_))\n",
        "\n",
        "# -------------------------\n",
        "# Summary\n",
        "# -------------------------\n",
        "print('\\n==== Summary of Deep Learning Models ====')\n",
        "summary = []\n",
        "summary.append({'Model':'Dense (TF-IDF)', 'Accuracy': accuracy_score(y_test, y_pred_dense)})\n",
        "summary.append({'Model':'1D CNN', 'Accuracy': accuracy_score(y_test, y_pred_cnn)})\n",
        "summary.append({'Model':'BiLSTM', 'Accuracy': accuracy_score(y_test, y_pred_lstm)})\n",
        "\n",
        "summary_df = pd.DataFrame(summary).sort_values(by='Accuracy', ascending=False)\n",
        "print(summary_df)\n",
        "\n",
        "# Save tokenizer and vectorizer for later use\n",
        "import pickle\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print('\\nSaved tokenizer.pkl and tfidf_vectorizer.pkl to current directory.')"
      ],
      "metadata": {
        "id": "l6MxLFej8vaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reinforcement Learning"
      ],
      "metadata": {
        "id": "T7Fm1VYE9ll9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "XVKQbKTn73q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinforcement Learning (Contextual Bandits) for Fake News Detection\n",
        "# -----------------------------------------------------------------\n",
        "# Problem framing: We treat each news article as a \"context\" (features from text),\n",
        "# and the agent chooses one of two actions: predict REAL (1) or FAKE (0).\n",
        "# Reward = 1 if the action matches the ground-truth label, else 0.\n",
        "# We compare several RL-style bandit policies:\n",
        "#  - LinUCB (contextual UCB)\n",
        "#  - Linear Thompson Sampling (contextual TS)\n",
        "#  - UCB1 (non-contextual baseline)\n",
        "# We also report cumulative reward and final test accuracy of the learned policies.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ------------------------\n",
        "# Load & featurize dataset\n",
        "# ------------------------\n",
        "\n",
        "df = pd.read_csv(\"/content/fake_news_dataset.csv\")\n",
        "# Combine title + text\n",
        "texts = (df[\"title\"].astype(str) + \" \" + df[\"text\"].astype(str)).tolist()\n",
        "labels = df[\"label\"].map({\"real\": 1, \"fake\": 0}).values  # actions: 1=REAL, 0=FAKE\n",
        "\n",
        "# Train/test split for offline evaluation of learned policies\n",
        "X_train_txt, X_test_txt, y_train, y_test = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# TF-IDF -> SVD (dense, low-d)\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_txt)\n",
        "X_test_tfidf = vectorizer.transform(X_test_txt)\n",
        "\n",
        "X_train = svd.fit_transform(X_train_tfidf)  # shape: (n_train, d)\n",
        "X_test = svd.transform(X_test_tfidf)        # shape: (n_test, d)\n",
        "\n",
        "d = X_train.shape[1]\n",
        "ACTIONS = [0, 1]  # 0=FAKE, 1=REAL\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Contextual Bandit: LinUCB and Linear Thompson Sam\n",
        "# -------------------------------------------------\n",
        "\n",
        "class LinUCB:\n",
        "    def _init_(self, d, n_actions=2, alpha=0.25):\n",
        "        self.alpha = alpha\n",
        "        self.n_actions = n_actions\n",
        "        # For each arm a: A[a] (dxd), b[a] (dx1)\n",
        "        self.A = [np.eye(d) for _ in range(n_actions)]\n",
        "        self.b = [np.zeros(d) for _ in range(n_actions)]\n",
        "\n",
        "    def theta(self, a):\n",
        "        A_inv = np.linalg.inv(self.A[a])\n",
        "        return A_inv @ self.b[a]\n",
        "\n",
        "    def select(self, x):\n",
        "        # UCB score: x^T theta_a + alpha * sqrt(x^T A_a^{-1} x)\n",
        "        x = x.reshape(-1)\n",
        "        best_a, best_score = None, -1e18\n",
        "        for a in range(self.n_actions):\n",
        "            A_inv = np.linalg.inv(self.A[a])\n",
        "            theta_a = A_inv @ self.b[a]\n",
        "            mean = x @ theta_a\n",
        "            conf = self.alpha * np.sqrt(x @ A_inv @ x)\n",
        "            score = mean + conf\n",
        "            if score > best_score:\n",
        "                best_score, best_a = score, a\n",
        "        return best_a\n",
        "\n",
        "    def update(self, x, a, r):\n",
        "        x = x.reshape(-1)\n",
        "        self.A[a] += np.outer(x, x)\n",
        "        self.b[a] += r * x\n",
        "\n",
        "class LinearThompsonSampling:\n",
        "    def _init_(self, d, n_actions=2, v=1.0, seed=42):\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.v = v\n",
        "        self.n_actions = n_actions\n",
        "        self.A = [np.eye(d) for _ in range(n_actions)]\n",
        "        self.b = [np.zeros(d) for _ in range(n_actions)]\n",
        "\n",
        "    def sample_theta(self, a):\n",
        "        A_inv = np.linalg.inv(self.A[a])\n",
        "        mu = A_inv @ self.b[a]\n",
        "        cov = (self.v ** 2) * A_inv\n",
        "        return self.rng.multivariate_normal(mu, cov)\n",
        "\n",
        "    def select(self, x):\n",
        "        x = x.reshape(-1)\n",
        "        best_a, best_val = None, -1e18\n",
        "        for a in range(self.n_actions):\n",
        "            theta_tilde = self.sample_theta(a)\n",
        "            val = x @ theta_tilde\n",
        "            if val > best_val:\n",
        "                best_val, best_a = val, a\n",
        "        return best_a\n",
        "\n",
        "    def update(self, x, a, r):\n",
        "        x = x.reshape(-1)\n",
        "        self.A[a] += np.outer(x, x)\n",
        "        self.b[a] += r * x\n",
        "\n",
        "class UCB1:\n",
        "    # Non-contextual baseline that ignores x; learns average reward per arm.\n",
        "    def _init_(self, n_actions=2):\n",
        "        self.n_actions = n_actions\n",
        "        self.counts = np.zeros(n_actions, dtype=int)\n",
        "        self.rewards = np.zeros(n_actions, dtype=float)\n",
        "        self.t = 0\n",
        "\n",
        "    def select(self, x):  # x unused\n",
        "        self.t += 1\n",
        "        # Play each arm once first\n",
        "        for a in range(self.n_actions):\n",
        "            if self.counts[a] == 0:\n",
        "                return a\n",
        "        # UCB1\n",
        "        avg = self.rewards / self.counts\n",
        "        bonus = np.sqrt(2 * np.log(self.t) / self.counts)\n",
        "        return int(np.argmax(avg + bonus))\n",
        "\n",
        "    def update(self, x, a, r):\n",
        "        self.counts[a] += 1\n",
        "        self.rewards[a] += r\n",
        "\n",
        "# ------------------------------\n",
        "# Online training (bandit loop)\n",
        "# ------------------------------\n",
        "\n",
        "# Shuffle training data for online simulation\n",
        "perm = np.random.RandomState(42).permutation(len(X_train))\n",
        "X_online = X_train[perm]\n",
        "y_online = y_train[perm]\n",
        "\n",
        "agents = {\n",
        "    \"LinUCB(alpha=0.25)\": LinUCB(d=d, alpha=0.25),\n",
        "    \"LinearTS(v=1.0)\": LinearThompsonSampling(d=d, v=1.0),\n",
        "    \"UCB1 (no context)\": UCB1(),\n",
        "}\n",
        "\n",
        "history = {name: {\"rewards\": [], \"actions\": [], \"correct\": []} for name in agents}\n",
        "\n",
        "for t in range(len(X_online)):\n",
        "    x = X_online[t]\n",
        "    y_true = y_online[t]\n",
        "    for name, agent in agents.items():\n",
        "        a = agent.select(x)\n",
        "        r = 1.0 if a == y_true else 0.0\n",
        "        agent.update(x, a, r)\n",
        "        history[name][\"rewards\"].append(r)\n",
        "        history[name][\"actions\"].append(a)\n",
        "        history[name][\"correct\"].append(int(r))\n",
        "\n",
        "# ------------------------------\n",
        "# Evaluate learned policies\n",
        "# ------------------------------\n",
        "\n",
        "def policy_from_lin_agent(agent):\n",
        "    # Build deterministic greedy policy from learned theta for each arm\n",
        "    thetas = [np.linalg.inv(agent.A[a]) @ agent.b[a] for a in range(agent.n_actions)]\n",
        "    thetas = np.stack(thetas)  # shape: (n_actions, d)\n",
        "    def policy(X):\n",
        "        # choose argmax_a x^T theta_a\n",
        "        scores = X @ thetas.T  # (n_samples, n_actions)\n",
        "        return np.argmax(scores, axis=1)\n",
        "    return policy\n",
        "\n",
        "# Build policies (LinUCB & LinearTS); UCB1 has no context so predict majority arm\n",
        "policies = {}\n",
        "for name, agent in agents.items():\n",
        "    if isinstance(agent, (LinUCB, LinearThompsonSampling)):\n",
        "        policies[name] = policy_from_lin_agent(agent)\n",
        "    else:\n",
        "        # Predict the arm with higher empirical reward\n",
        "        maj_arm = int(np.argmax(agent.rewards / np.maximum(agent.counts, 1)))\n",
        "        policies[name] = lambda X, arm=maj_arm: np.full(X.shape[0], arm, dtype=int)\n",
        "\n",
        "results = []\n",
        "for name, policy in policies.items():\n",
        "    y_pred = policy(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append({\n",
        "        \"Agent\": name,\n",
        "        \"Cumulative Reward (train)\": int(np.sum(history[name][\"rewards\"])),\n",
        "        \"Train Accuracy (online)\": float(np.mean(history[name][\"correct\"])) ,\n",
        "        \"Test Accuracy\": acc\n",
        "    })\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(f\"Cumulative Reward (train): {int(np.sum(history[name]['rewards']))}\")\n",
        "    print(f\"Train Accuracy (online): {np.mean(history[name]['correct']):.4f}\")\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"Fake\", \"Real\"]))\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n==== Summary ====\")\n",
        "print(results_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "agnKypuJ9qR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}